---
layout: post
title: Notes on Text-To-Speech System
author: hoangbm
---

Recently, I've spent quite a lot of time to study about Text-to-Speech (TTS) system, more specifically, acoustic model. To be fair, this module is the most actively researched in the whole pipeline. In the beginning, admittedly, I'm not a big fan of speech processing because of the vagueness of some terms like prosody, timbre. But I think I like it more now. Below are some notes about TTS pipeline during the process.

### Key components of TTS system

In modern TTS system, there are arguably 3 main modules which help to transform text to sound waveform that we can hear and understand:

1. Text analysis does text normalization and graphene-to-phoneme conversion. Text normalization is to clean the input text and replace number/abbreviation with their standard word format. Then, graphene-to-phoneme is to get the phoneme representation from the the word format.

2. Acoustic model converts text features to melspectrogram. Melspectrogram is chosen since it emphasizes details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing higher frequencies, which are dominated by noise burst. This is supposedly the most important module in TTS system due to the task complexity and data shortage.

3. Vocoder model transforms melspectrogram into speech waveform. Personally, I don't think it's difficult to train since data is abundant, we have algorithm to convert speech waveform to melspectrogram.

### Alignment in TTS system

### Advanced topic in TTS system

### TTS system

- TTS can be an end-to-end system which transform text to speech directly. However, cascading system is more mature and prominent. It comprises of 2 components: melspectrogram predictor and vocoder.
- Melspectrogram is a kind of image-like data which describes the acoustic features of sound. Normally, once you have the audio file, you can use some deterministic algorithms to produce melspectrogram. Nevertheless, from text to melspectrogram, you need a machine learning model to do that. Melspectrogram will determine almost every aspect of voice like pitch, energy, etc.
- Vocoder will do the reverse transformation from melspectrogram to audio. This module will decide the audio quality like how noisy it is. The training data for this module can be considered illimited, so this module is quite mature. We don't have to train this model as frequently as the former.

### Auto-regressive versus non-autoregressive

- Auto-regressive models like Tacotron will treat the problem like language translation in NLP. Tacotron uses Encoder module to capture the latent representation of the whole text, then uses Decoder to predict the next melspectrogram frame, then append it to the current collection. Furthermore, it also predict the score to decide whether we stop at the current step or not.

- Non-autoregressive models like Fastspeech will predict the whole frame sequence at once. It can do that because it also has a module to predict the length of the frame. Obviously, it is much faster than the former.